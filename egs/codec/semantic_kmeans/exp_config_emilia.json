{
    "model_type": "KMeans",
    "dataset": ["emilia"],
    "preprocess": {
        "hop_size": 200,
        "sample_rate": 16000,
        "processed_dir": "",
        "valid_file": "valid.json",
        "train_file": "train.json"
    },
    "model": {
        "type": "kmeans_ema",
        "stat_mean_var_path": "/mnt/bn/yuacnwang-speech/ckpt/semantic_kmeans/emilia_wav2vec2bert_stats_10k.pt",
        "kmeans": {
            "codebook_size": 2048,
            "codebook_dim": 1024,
            "kmeans_init": true,
            "kmeans_iters": 10,
            "decay": 0.92,
            "eps": 1e-5
        }
    },
    "log_dir": "/mnt/bn/yuacnwang-speech/ckpt/semantic_kmeans",
    "train": {
        "max_epoch": 0,
        "use_dynamic_batchsize": true,
        "max_tokens": 12000000,
        "max_sentences": 150,
        "lr_warmup_steps": 10000,
        "lr_scheduler": "constant",
        "num_train_steps": 100000,
        "adam": {
            "lr": 1e-4
        },
        "ddp": false,
        "random_seed": 114,
        "batch_size": 10,
        "epochs": 5000,
        "max_steps": 1000000,
        "total_training_steps": 800000,
        "save_summary_steps": 500,
        "save_checkpoints_steps": 1000,
        "valid_interval": 2000,
        "keep_checkpoint_max": 100,
        "gradient_accumulation_step": 1,
        "tracker": ["tensorboard"],
        "save_checkpoint_stride": [1],
        "keep_last": [5],
        "run_eval": [true],
        "dataloader": {
          "num_worker": 32,
          "pin_memory": true
        },
        "use_emilia_dataset": true
    }
}