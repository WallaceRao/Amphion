{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.codec.kmeans.kmeans_model import KMeans, KMeansEMA\n",
    "from models.tts.soundstorm.soundstorm_model import SoundStorm\n",
    "from models.codec.amphion_codec.codec import CodecEncoder, CodecDecoder\n",
    "from models.tts.text2semantic.t2s_model import T2SLlama\n",
    "from transformers import Wav2Vec2BertModel\n",
    "import safetensors\n",
    "from utils.util import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.g2p_new.g2p import phonemizer_g2p\n",
    "LANG2CODE = {\n",
    "    'zh': 349,\n",
    "    'en': 350,\n",
    "    'ja': 351,\n",
    "    'ko': 352,\n",
    "    'fr': 353,\n",
    "    'de': 354,\n",
    "}\n",
    "def g2p(text, language):\n",
    "    return phonemizer_g2p(text, language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"/opt/tiger/SpeechGeneration/egs/tts/SoundStorm/exp_config_16k_emilia_llama.json\")\n",
    "t2s_cfg = load_config(\"/opt/tiger/SpeechGeneration/egs/tts/Text2Semantic/exp_config_16k_emilia.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_soundstorm(cfg, pretrained_path, device):\n",
    "    soundstorm_model = SoundStorm(cfg=cfg.model.soundstorm)\n",
    "    if \".bin\" in pretrained_path:\n",
    "        soundstorm_model .load_state_dict(torch.load(pretrained_path))\n",
    "    elif \".safetensors\" in pretrained_path:\n",
    "        safetensors.torch.load_model(soundstorm_model, pretrained_path)\n",
    "    soundstorm_model.eval()\n",
    "    soundstorm_model.to(device)\n",
    "    return soundstorm_model\n",
    "\n",
    "def build_kmeans_model(cfg, device):\n",
    "    if cfg.model.kmeans.type == \"kmeans\":\n",
    "        kmeans_model = KMeans(cfg=cfg.model.kmeans.kmeans)\n",
    "    elif cfg.model.kmeans.type == \"kmeans_ema\":\n",
    "        kmeans_model = KMeansEMA(cfg=cfg.model.kmeans.kmeans)\n",
    "    kmeans_model.eval()\n",
    "    pretrained_path =cfg.model.kmeans.pretrained_path\n",
    "    if \".bin\" in pretrained_path:\n",
    "        kmeans_model.load_state_dict(torch.load(pretrained_path))\n",
    "    elif \".safetensors\" in pretrained_path:\n",
    "        safetensors.torch.load_model(kmeans_model, pretrained_path)\n",
    "    kmeans_model.to(device)\n",
    "    return kmeans_model\n",
    "\n",
    "def build_semantic_model(cfg, device):\n",
    "    semantic_model = Wav2Vec2BertModel.from_pretrained(\"facebook/w2v-bert-2.0\")\n",
    "    semantic_model.eval()\n",
    "    semantic_model.to(device)\n",
    "    # layer_idx = 15\n",
    "    # if layer_idx == 23:\n",
    "    #     output_idx = 0\n",
    "    # else:\n",
    "    #     output_idx = layer_idx + 2\n",
    "    layer_idx = 15\n",
    "    output_idx = 17\n",
    "    stat_mean_var = torch.load(cfg.model.kmeans.stat_mean_var_path)\n",
    "    semantic_mean = stat_mean_var[\"mean\"]\n",
    "    semantic_std = torch.sqrt(stat_mean_var[\"var\"])\n",
    "    semantic_mean = semantic_mean.to(device)\n",
    "    semantic_std = semantic_std.to(device)\n",
    "    # print(\n",
    "    #     \"semantic mean: \", semantic_mean, \"semantic std: \", semantic_std\n",
    "    # )\n",
    "    return semantic_model, semantic_mean, semantic_std\n",
    "\n",
    "def build_codec_model(cfg, device):\n",
    "    codec_encoder = CodecEncoder(cfg=cfg.model.codec.encoder)\n",
    "    codec_decoder = CodecDecoder(cfg=cfg.model.codec.decoder)\n",
    "    codec_encoder.load_state_dict(\n",
    "        torch.load(cfg.model.codec.encoder.pretrained_path)\n",
    "    )\n",
    "    codec_decoder.load_state_dict(\n",
    "        torch.load(cfg.model.codec.decoder.pretrained_path)\n",
    "    )\n",
    "    # codec_decoder = codec_decoder.quantizer  # we only need the quantizer\n",
    "    codec_encoder.eval()\n",
    "    codec_decoder.eval()\n",
    "    codec_encoder.to(device)\n",
    "    codec_decoder.to(device)\n",
    "    return codec_encoder, codec_decoder\n",
    "\n",
    "def build_t2s_model(cfg, device):\n",
    "    t2s_model = T2SLlama(cfg=cfg.model.t2sllama)\n",
    "    t2s_model.eval()\n",
    "    t2s_model.to(device)\n",
    "    return t2s_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")\n",
    "soundstorm_pretrained_path = \"/mnt/bn/yuacnwang-speech/ckpt/soundstorm/soundstorm_16k_kmeans_2048_emilia_50k_llama/checkpoint/epoch-0011_step-0123000_loss-4.518502/model.safetensors\"\n",
    "soundstorm_model = build_soundstorm(cfg, soundstorm_pretrained_path, device)\n",
    "semantic_model, semantic_mean, semantic_std = build_semantic_model(cfg, device)\n",
    "kmeans_model = build_kmeans_model(cfg, device)\n",
    "codec_encoder, codec_decoder = build_codec_model(cfg, device)\n",
    "t2s_model = build_t2s_model(t2s_cfg, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_mean = semantic_mean.to(device)\n",
    "semantic_std = semantic_std.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safetensors.torch.load_model(soundstorm_model, \"/mnt/bn/yuacnwang-speech/ckpt/soundstorm/soundstorm_16k_kmeans_2048_emilia_50k_llama/checkpoint/epoch-0011_step-0127000_loss-5.334249/model.safetensors\")\n",
    "safetensors.torch.load_model(t2s_model, \"/mnt/bn/yuacnwang-speech/ckpt/text2semantic/t2s_16k_kmeans_2048_emilia_50k/checkpoint/epoch-0001_step-0076000_loss-1.535404/model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SeamlessM4TFeatureExtractor\n",
    "processor = SeamlessM4TFeatureExtractor.from_pretrained(\"facebook/w2v-bert-2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_acoustic_code(speech):\n",
    "    vq_emb = codec_encoder(speech.unsqueeze(1))\n",
    "    _, vq, _, _, _ = codec_decoder.quantizer(vq_emb)\n",
    "    acoustic_code = vq.permute(\n",
    "        1, 2, 0\n",
    "    )  # (num_quantizer, T, C) -> (T, C, num_quantizer)\n",
    "    return acoustic_code\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_semantic_code(semantic_mean, semantic_std, input_features, attention_mask):\n",
    "    vq_emb = semantic_model(\n",
    "        input_features=input_features,\n",
    "        attention_mask=attention_mask,\n",
    "        output_hidden_states=True,\n",
    "    )\n",
    "    feat = vq_emb.hidden_states[17]  # (B, T, C)\n",
    "    feat = (feat - semantic_mean.to(feat)) / semantic_std.to(feat)\n",
    "\n",
    "    semantic_code, _ = kmeans_model.quantize(feat)  # (B, T)\n",
    "    return semantic_code\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(speech, processor):\n",
    "    inputs = processor(speech, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_features = inputs[\"input_features\"][0]\n",
    "    attention_mask = inputs[\"attention_mask\"][0]\n",
    "    return input_features, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/mnt/bn/yuacnwang-speech/dataset/Emilia/emilia/emilia_50k/wav\"\n",
    "dataset_wav_info = []\n",
    "for subset in os.listdir(dataset_path):\n",
    "    for wav in os.listdir(os.path.join(dataset_path, subset)):\n",
    "        wav_path = os.path.join(dataset_path, subset, wav)\n",
    "        dataset_wav_info.append(wav_path)\n",
    "print(len(dataset_wav_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = 888\n",
    "wav_path = dataset_wav_info[random_idx]\n",
    "meta_path = wav_path.replace(\"/wav/\", \"/meta/\").replace(\".wav\", \".pkl\")\n",
    "with open(meta_path, 'rb') as f:\n",
    "    meta_info = pickle.load(f)\n",
    "uid = wav_path.split(\"/\")[-1].split(\".\")[0]\n",
    "print(uid)\n",
    "speech, sr = librosa.load(wav_path, sr=16000)\n",
    "print(meta_info)\n",
    "Audio(speech, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_id = g2p(meta_info['text'], meta_info['language'])[1]\n",
    "phone_id = torch.tensor(phone_id, dtype=torch.long)\n",
    "phone_id = torch.cat([torch.tensor(LANG2CODE[meta_info['language']], dtype=torch.long).reshape(1), phone_id]).to(device) # add language token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = meta_info[\"text\"]\n",
    "with open(\"/mnt/bn/yuacnwang-speech/test_result/soundstorm_kmeans_2048_ar_76k_nar_127k/coninue/txt/{}.txt\".format(uid), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_path = \"/mnt/bn/yuacnwang-speech/dataset/temp_test/ns2_10.wav\"\n",
    "speech, sr = librosa.load(wav_path, sr=16000)\n",
    "uid = wav_path.split(\"/\")[-1].split(\".\")[0]\n",
    "text = \"For a few miles, she followed the line hitherto presumably occupied by the coast of Algeria, but no land appeared to the south.\"\n",
    "Audio(speech, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_id = g2p(text, 'en')[1]\n",
    "phone_id = torch.tensor(phone_id, dtype=torch.long)\n",
    "phone_id = torch.cat([torch.tensor(LANG2CODE[meta_info['language']], dtype=torch.long).reshape(1), phone_id]).to(device) # add language token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fetures, attention_mask = extract_features(speech, processor)\n",
    "input_fetures = input_fetures.unsqueeze(0).to(device)\n",
    "attention_mask = attention_mask.unsqueeze(0).to(device)\n",
    "semantic_code = extract_semantic_code(semantic_mean, semantic_std, input_fetures, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SoundStorm Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_code = extract_acoustic_code(torch.tensor(speech).unsqueeze(0).to(device))\n",
    "seq_len = min(semantic_code.shape[1], acoustic_code.shape[1])\n",
    "semantic_code = semantic_code[:, :seq_len]\n",
    "acoustic_code = acoustic_code[:, :seq_len, :]\n",
    "cond = soundstorm_model.cond_emb(semantic_code.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cond = soundstorm_model.cond_emb(semantic_code.to(device))\n",
    "# prompt = acoustic_code[:,:50*3,:]\n",
    "# predict = soundstorm_model.reverse_diffusion(cond=cond.to(device), prompt=prompt.to(device), temp=1.5, filter_thres=0.98, n_timesteps=[50, 10, 1, 1, 1, 1, 1, 1], cfg=1.0, rescale_cfg=1.0, phone_id=phone_id.unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vq_emb = codec_decoder.vq2emb(predict.permute(2,0,1))\n",
    "# recovered_audio = codec_decoder(vq_emb)\n",
    "# recovered_audio = recovered_audio[0][0].cpu().detach().numpy()\n",
    "# Audio(recovered_audio, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_vq_emb = codec_decoder.vq2emb(prompt.permute(2,0,1))\n",
    "# recovered_prompt_audio = codec_decoder(prompt_vq_emb)\n",
    "# recovered_prompt_audio = recovered_prompt_audio[0][0].cpu().detach().numpy()\n",
    "# Audio(recovered_prompt_audio, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_audio = np.concatenate([recovered_prompt_audio, recovered_audio])\n",
    "# Audio(combine_audio, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SoundStorm Continue TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_len = 50*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_semantic = t2s_model.sample_hf(phone_ids=phone_id.unsqueeze(0), prompt_ids=semantic_code[:, :prompt_len], temperature=1.0, top_k=100, top_p=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_semantic_code = torch.cat([semantic_code[:,:prompt_len], predict_semantic], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_code = extract_acoustic_code(torch.tensor(speech).unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = soundstorm_model.cond_emb(combine_semantic_code)\n",
    "print(cond.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = acoustic_code[:,:prompt_len,:]\n",
    "predict = soundstorm_model.reverse_diffusion(cond=cond, prompt=prompt, temp=1.5, filter_thres=0.98, n_timesteps=[50, 10, 1, 1, 1, 1, 1, 1], cfg=1.0, rescale_cfg=1.0)\n",
    "print(predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_emb = codec_decoder.vq2emb(predict.permute(2,0,1))\n",
    "recovered_audio = codec_decoder(vq_emb)\n",
    "recovered_audio = recovered_audio[0][0].cpu().detach().numpy()\n",
    "sf.write(\"/mnt/bn/yuacnwang-speech/test_result/soundstorm_kmeans_2048_ar_76k_nar_127k/coninue/target/{}.wav\".format(uid), recovered_audio, 16000)\n",
    "Audio(recovered_audio, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_vq_emb = codec_decoder.vq2emb(prompt.permute(2,0,1))\n",
    "recovered_prompt_audio = codec_decoder(prompt_vq_emb)\n",
    "recovered_prompt_audio = recovered_prompt_audio[0][0].cpu().detach().numpy()\n",
    "sf.write(\"/mnt/bn/yuacnwang-speech/test_result/soundstorm_kmeans_2048_ar_76k_nar_127k/coninue/prompt/{}.wav\".format(uid), recovered_prompt_audio, 16000)\n",
    "Audio(recovered_prompt_audio, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_audio = np.concatenate([recovered_prompt_audio, recovered_audio])\n",
    "sf.write(\"/mnt/bn/yuacnwang-speech/test_result/soundstorm_kmeans_2048_ar_76k_nar_127k/coninue/combine/{}.wav\".format(uid), combine_audio, 16000)\n",
    "Audio(combine_audio, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SoundStorm Cross TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_wav_path = \"/mnt/bn/yuacnwang-speech/dataset/temp_test/biden_2.wav\"\n",
    "prompt_speech, sr = librosa.load(prompt_wav_path, sr=16000)\n",
    "uid = prompt_wav_path.split(\"/\")[-1].split(\".\")[0]\n",
    "prompt_text = \"We do not break, we never give in, we never backdown.\"\n",
    "prompt_phone_id = g2p(prompt_text, 'en')[1]\n",
    "prompt_phone_id = torch.tensor(prompt_phone_id, dtype=torch.long)\n",
    "prompt_phone_id = torch.cat([torch.tensor(LANG2CODE['en'], dtype=torch.long).reshape(1), prompt_phone_id])\n",
    "target_text = \"我的名字叫做拜登\" \n",
    "target_phone_id = g2p(target_text, 'zh')[1]\n",
    "target_phone_id = torch.tensor(target_phone_id, dtype=torch.long)\n",
    "# target_phone_id = torch.cat([torch.tensor(LANG2CODE['en'], dtype=torch.long).reshape(1), target_phone_id])\n",
    "phone_id = torch.cat([prompt_phone_id, target_phone_id])\n",
    "device = torch.device(\"cuda:1\")\n",
    "phone_id = phone_id.to(device)\n",
    "text = prompt_text + target_text\n",
    "with open(\"/mnt/bn/yuacnwang-speech/test_result/soundstorm_kmeans_2048_ar_76k_nar_127k/cross/txt/{}.txt\".format(uid), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(target_text)\n",
    "Audio(prompt_speech, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fetures, attention_mask = extract_features(prompt_speech, processor)\n",
    "input_fetures = input_fetures.unsqueeze(0).to(device)\n",
    "attention_mask = attention_mask.unsqueeze(0).to(device)\n",
    "prompt_semantic_code = extract_semantic_code(semantic_mean, semantic_std, input_fetures, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_semantic = t2s_model.sample_hf(phone_ids=phone_id.unsqueeze(0), prompt_ids=prompt_semantic_code, temperature=0.95, top_k=512, top_p=0.9)\n",
    "semantic_code = torch.cat([prompt_semantic_code, predict_semantic], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_code = extract_acoustic_code(torch.tensor(prompt_speech).unsqueeze(0).to(device))\n",
    "print(acoustic_code.shape)\n",
    "cond = soundstorm_model.cond_emb(semantic_code.to(device))\n",
    "print(cond.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = acoustic_code\n",
    "predict = soundstorm_model.reverse_diffusion(cond=cond, prompt=prompt, temp=1.5, filter_thres=0.98, n_timesteps=[50, 10, 1, 1, 1, 1, 1, 1], cfg=1.0, rescale_cfg=1.0)\n",
    "print(predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_emb = codec_decoder.vq2emb(predict.permute(2,0,1))\n",
    "recovered_audio = codec_decoder(vq_emb)\n",
    "recovered_audio = recovered_audio[0][0].cpu().detach().numpy()\n",
    "sf.write(\"/mnt/bn/yuacnwang-speech/test_result/soundstorm_kmeans_2048_ar_76k_nar_127k/cross/target/{}.wav\".format(uid), recovered_audio, 16000)\n",
    "Audio(recovered_audio, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_vq_emb = codec_decoder.vq2emb(prompt.permute(2,0,1))\n",
    "recovered_prompt_audio = codec_decoder(prompt_vq_emb)\n",
    "recovered_prompt_audio = recovered_prompt_audio[0][0].cpu().detach().numpy()\n",
    "sf.write(\"/mnt/bn/yuacnwang-speech/test_result/soundstorm_kmeans_2048_ar_76k_nar_127k/cross/prompt/{}.wav\".format(uid), recovered_prompt_audio, 16000)\n",
    "Audio(recovered_prompt_audio, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_audio = np.concatenate([recovered_prompt_audio, recovered_audio])\n",
    "sf.write(\"/mnt/bn/yuacnwang-speech/test_result/soundstorm_kmeans_2048_ar_76k_nar_127k/cross/combine/{}.wav\".format(uid), combine_audio, 16000)\n",
    "Audio(combine_audio, rate=16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
